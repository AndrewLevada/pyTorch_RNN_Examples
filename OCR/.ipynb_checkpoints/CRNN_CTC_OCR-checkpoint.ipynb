{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  Training an OCR using CNN + RNN + CTC on Synthetic Images ##\n",
    "- Here we change the network architecture slightly from RNN+CTC ; we add a convolutional stack before the BLSTM layer\n",
    "    - Now your input to the network is not the raw pixel values, But we do steps of convolution and maxpooling and the resultant output is reshpaed to form a Time x featDim structured before it is fed to the network. \n",
    "    - The convoultional stack can be increased in depth to get better feature represenations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Compared to the RNN+CTC code, the only change here is in the model definition part where we have a convolutional stack ahead of the BLSTM stack</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Use a BRNN + CTC to recognize given word image \n",
    "# Network is trained on images rendered using PIL \n",
    "# ============================================================================\n",
    "# \n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "from PIL import Image, ImageFont, ImageDraw, ImageEnhance\n",
    "import numpy as np\n",
    "import time,math\n",
    "from time import sleep\n",
    "import random\n",
    "import sys,codecs,glob \n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from warpctc_pytorch import CTCLoss\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "random.seed(0)\n",
    "# TODO - MAKE SURE CTC IS INSTALLED IN ALL MACHINES\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    print ('CUDA is available')\n",
    "\n",
    "#use_cuda=False   #uncomment this if you dont want to use cuda variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vocabulary and the fonts ####\n",
    "-  loading the lexicon of 90k words\n",
    "- get the fontslist to be used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#all word images are resized to a height of 32 pixels\n",
    "imHeight=32 \n",
    "\"\"\"\n",
    "image width is also set a fixed size\n",
    "YES. Though RNNS can handle variable length sequences we resize them to fixed width\n",
    "This is for the ease of batch learning\n",
    "\n",
    "\"\"\"\n",
    "#imWidth=100\n",
    "imWidth=100\n",
    "#13 fonts from googlefonts is used\n",
    "#incase you want to use different set of fonts, change the path below\n",
    "fontsList=glob.glob('fontsForRendering/'+'*.ttf')\n",
    "# a  90k size lexicon \n",
    "# lexicon source : http://www.robots.ox.ac.uk/~vgg/data/text/\n",
    "vocabFile=codecs.open('lexicon.txt','r')\n",
    "#90k vocabulary\n",
    "words = vocabFile.read().split()\n",
    "vocabSize=len(words)\n",
    "fontSizeOptions={'16','20','24','28','30','32','36','38'}\n",
    "batchSize=5 \n",
    "alphabet='0123456789abcdefghijklmnopqrstuvwxyz-'\n",
    "#alphabet=\"(3)-\"\n",
    "dict={}\n",
    "for i, char in enumerate(alphabet):\n",
    "\tdict[char] = i + 1\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## a simple helper function to compute time since some 'start time'\n",
    "def time_since(since):\n",
    "\ts = time.time() - since\n",
    "\tm = math.floor(s / 60)\n",
    "\ts -= m * 60\n",
    "\treturn '%dm %ds' % (m, s)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# return the class labels for each character in the targetsequence \n",
    "def Str2Labels(text):\n",
    "\tglobal dict\n",
    "\ttext = [dict[char.lower()] for char in text]\n",
    "\t#print (text)\n",
    "\tlength=len(text)\n",
    "\treturn text, length\n",
    "#StrtoLabels(\"0-1\")\n",
    "\n",
    "### from the predicted sequence of labels for an image, decode the string\n",
    "# function returns the rawstring and also the decoded string after removing blanks and duplicates\n",
    "\n",
    "#eg: if labelsequnce you get after an argmax on the output activation matris is  [12,12,0,0,15,0,15,15,0,0]\n",
    "#then your raw label string would be \"bb~~e~ee~~\" and the outputstring \"bee\"\n",
    "def Labels2Str(predictedLabelSequences):\n",
    "    bz=predictedLabelSequences.size(0)\n",
    "    predictedRawStrings=[]\n",
    "    predictedStrings=[]\n",
    "    for i in range(0,bz):\n",
    "        predictedRawString=\"\"\n",
    "        predictedString=\"\"\n",
    "        predictedLabelSeq=predictedLabelSequences.data[i,:]\n",
    "        prevId=1000 #just a large value which is not in the index \n",
    "        character=\"\"\n",
    "        character_raw=\"\"\n",
    "        for j in range (0, predictedLabelSeq.size(0)):\n",
    "            idx=predictedLabelSeq[j]\n",
    "            if (prevId != 1000 or prevId!=idx) :\n",
    "                if prevId!=idx:\n",
    "                    if idx==0:\n",
    "                        character_raw=\"~\"\n",
    "                        character=\"\"\n",
    "                    else:\n",
    "                        character_raw=alphabet[idx-1]\n",
    "                        character=alphabet[idx-1]\n",
    "                else:\n",
    "                    character_raw=\"~\"\n",
    "                    character=\"\"\n",
    "                prevId=idx\n",
    "            else:\n",
    "                character=\"\"\n",
    "                if idx==0:\n",
    "                    character_raw=\"~\"\n",
    "                else:\n",
    "                    character_raw=alphabet[idx-1]\n",
    "                    \n",
    "                    \n",
    "\n",
    "            \n",
    "            predictedString+=character\n",
    "            predictedRawString+=character_raw\n",
    "        predictedRawStrings.append(predictedRawString)\n",
    "        predictedStrings.append(predictedString)\n",
    "        \n",
    "    return predictedRawStrings, predictedStrings\n",
    "\n",
    "\n",
    "\n",
    "def image2tensor(im):\n",
    "    #returns the pixel values of a PIL image (in 0-1 range) as a numpy 2D array\n",
    "\n",
    "    (width, height) = im.size\n",
    "    greyscale_map = list(im.getdata())\n",
    "    greyscale_map = np.array(greyscale_map, dtype = np.uint8)\n",
    "    greyscale_map=greyscale_map.astype(float)\n",
    "    greyscale_map = torch.from_numpy(greyscale_map.reshape((height, width))).float()/255.0\n",
    "    return greyscale_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render the images, prepare a training batch ###\n",
    "- renders a batch of word images, from the list of words supplied\n",
    "- if singleFont is true then only one font would be used to render images. This is useful in case where you want to test overfitting the network to easy examples\n",
    "- Along with the rendered images, the target strings are converted to corresponding sequence of labels; for example the word \"bee\" would be converted to [12,15,15] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GetBatch ( batchOfWords ):\n",
    "\t\"\"\"\n",
    "\tRenders a batch of word images and returns the images along with the corresponding GTs\n",
    "\tUses PIL to render word images\n",
    "\tfont is randomly picked from a set of freely available google fonts\n",
    "\tword is picked from a vocabulary of English words\n",
    "\n",
    "\t\"\"\"\n",
    "\twordImages=[]\n",
    "\tlabelSequences=[]\n",
    "\tlabelSeqLengths=[]\n",
    "\n",
    "\tfor  i,text in enumerate (batchOfWords):\n",
    "\t\twordText=text\n",
    "\t\t#print('text is', text)\n",
    "\t\tfontName=fontsList[0]\n",
    "\t\tfontSize='26'\n",
    "\t\t#fontSize=fontSizeOptions[0]\n",
    "\t\tfontName=random.sample(fontsList,1)[0]\n",
    "\t\tfontSize=random.sample(fontSizeOptions,1)[0]\n",
    "\t\timageFont = ImageFont.truetype(fontName,int(fontSize))\n",
    "\t\ttextSize=imageFont.getsize(wordText)\n",
    "\t\timg=Image.new(\"L\", textSize,(255))\n",
    "\t\tdraw = ImageDraw.Draw(img)\n",
    "\t\tdraw.text((0, 0),wordText,(0),font=imageFont)\n",
    "\t\timg=img.resize((imWidth,imHeight), Image.ANTIALIAS)\n",
    "\t\t#img.save(text+'.jpeg')\n",
    "\n",
    "\t\timgTensor=image2tensor(img)\n",
    "\t\timgTensor=imgTensor.unsqueeze(0) # at 0 a new dimenion is added\n",
    "\n",
    "\t\twordImages.append(imgTensor)\n",
    "\n",
    "\t\tlabelSeq,l=Str2Labels(wordText)\n",
    "\t\tlabelSequences+=labelSeq\n",
    "\t\tlabelSeqLengths.append(l)\n",
    "\tbatchImageTensor=torch.cat(wordImages,0) #now all the image tensors are combined ( we  did the unsqueeze eariler for this cat)\t\n",
    "\t#batchImageTensor=torch.transpose(batchImageTensor,1,2)\n",
    "\tlabelSequencesTensor=torch.IntTensor(labelSequences)\n",
    "\tlabelSeqLengthsTensor=torch.IntTensor(labelSeqLengths)\n",
    "\treturn batchImageTensor, labelSequencesTensor, labelSeqLengthsTensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a convolutional stack to the BLSTM + CTC Architecure ###\n",
    "Remember that earlier we were feeding raw pixel columns as inputs at each time step <br>\n",
    "Here we will use a covolutional stack to get some useful represenations from the word image <br>\n",
    "And then sequences of this convolutional features are fed to the BLSTM layer above <br>\n",
    "\n",
    "![CRNN Architecture](crnnstack.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# minesh TODO split blstm into a separate class ?\n",
    "\n",
    "class crnnocr (nn.Module):\n",
    "    def __init__(self, inputDim, hiddenDim, outputDim,  numLayers, numDirections):\n",
    "        super(crnnocr, self).__init__()\n",
    "        self.inputDim=inputDim\n",
    "        self.hiddenDim=hiddenDim\n",
    "        self.outputDim=outputDim\n",
    "        self.numLayers=numLayers\n",
    "        self.numDirections=numDirections\n",
    "        # bidirectional= true to make the rnn bidirectional\n",
    "        #cnn stack\n",
    "        self.conv1 = nn.Conv2d(1, 64, 3)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3)\n",
    "        \n",
    "        \n",
    "        # rnn part\n",
    "        if numDirections==2:\n",
    "            self.blstm1=nn.LSTM(384, hiddenDim,numLayers, bidirectional=True, batch_first=True) # first blstm layer takes the image features as inputs\n",
    "        else:\n",
    "            self.blstm1=nn.LSTM(384, hiddenDim,numLayers, bidirectional=False, batch_first=True)\n",
    "        self.linearLayer2=nn.Linear(hiddenDim*numDirections, outputDim) # linear layer at the output\n",
    "        self.softmax = nn.Softmax()\n",
    "                \n",
    "    def forward(self, x ):\n",
    "        # x will be in shape B x D x T\n",
    "        x=x.unsqueeze(1) # # we add an extra dimension at 1 for #channels\n",
    "        #see the input dimension required for conv2s\n",
    "        #print(x.size())\n",
    "        \n",
    "        #print('size of x in the beginning =', x.size()) # batxhSizexnumChannels=1xHxW\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # batchSizex64xH/2-1(W/2-1)x\n",
    "        #print('size of x after conv1 and pooling =', x.size())\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2) # batchSizex64xH/2/2-1x(W/2-1)/2-1\n",
    "        #print('size of x after conv2 and pooling =', x.size())\n",
    "        #if input is 50x1x32x100 then it would become 50x64x16*49 and then 50x64x6x23\n",
    "        #print(x.size())\n",
    "        x=x.contiguous()\n",
    "        B,C,D,T=x.size(0), x.size(1), x.size(2), x.size(3)\n",
    "        #x=x.transpose(2,3) #swapping last two dimensions\n",
    "        x=x.contiguous()\n",
    "        x=x.view(B,x.size(1)*x.size(2),-1) # BxC*DXT\n",
    "        x=x.transpose(1,2) #making T the second dimension\n",
    "        #print(x.size())\n",
    "        \n",
    "        \n",
    "        lstmOut1, _  =self.blstm1(x ) #x has three dimensions batchSize* seqLen * FeatDim\n",
    "        B,T,D  = lstmOut1.size(0), lstmOut1.size(1), lstmOut1.size(2)\n",
    "        lstmOut1=lstmOut1.contiguous()\n",
    "\n",
    "                \n",
    "        # output of RNN is reshaped to B*T x D before it is fed to the linear layer\n",
    "        outputLayerActivations=self.linearLayer2(lstmOut1.view(B*T,D))\n",
    "        outputSoftMax=self.softmax(outputLayerActivations)\n",
    "        # the activations are reshaped to B x T x outputDim size\n",
    "        #then a transpose of B and T since CTC expects the T to be first\n",
    "        outputLayerActivations= outputLayerActivations.view(B,T,-1).transpose(0,1)\n",
    "        #if use_cuda:\n",
    "        #    outputLayerActivations=outputLayerActivations.cuda()\n",
    "        return outputLayerActivations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########\n",
    "# Prepare the synthetic validation data\n",
    "##############\n",
    "\n",
    "valWords=['azadi','from','beef','janata','party']\n",
    "valImages, valLabelSeqs, valLabelSeqlens=GetBatch(valWords)\n",
    "valImages=autograd.Variable(valImages)\n",
    "valImages=valImages.contiguous()\n",
    "if use_cuda:\n",
    "    valImages=valImages.cuda()\n",
    "valLabelSeqs=autograd.Variable(valLabelSeqs)\n",
    "#print(valLabelSeqs.data)\n",
    "valLabelSeqlens=autograd.Variable(valLabelSeqlens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ctc Cost on validation data is 67.3995513916\n",
      "m~~~~~~~~~~~~~~~~~~~~g~==>mg\n",
      "g~~~~~~~m~~~~~~~~g~mg~~==>gmgmg\n",
      "g~~m~~~~~~~~~~~~~~g~~~~==>gmg\n",
      "mg~~m~~~~~~~~~~~~~~g~~~==>mgmg\n",
      "m~~~~~~~~~~~~~~~~~g~~~~==>mg\n",
      "word error on validation data is 100\n",
      "Time since we began trainiing [0m 2s]\n",
      "ctc Cost on validation data is 18.230588913\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "word error on validation data is 100\n",
      "Time since we began trainiing [0m 3s]\n",
      "ctc Cost on validation data is 17.5130767822\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "word error on validation data is 100\n",
      "Time since we began trainiing [0m 4s]\n",
      "ctc Cost on validation data is 18.241809845\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "word error on validation data is 100\n",
      "Time since we began trainiing [0m 6s]\n",
      "ctc Cost on validation data is 18.9192466736\n",
      "~~~~~~~~~~~~~~~~~~~~~~s==>s\n",
      "~~~~~~~~~~~~~~~~~~~~~~s==>s\n",
      "~~~~~~~~~~~~~~~~~~~~~~s==>s\n",
      "~~~~~~~~~~~~~~~~~~~~~~s==>s\n",
      "~~~~~~~~~~~~~~~~~~~~~~s==>s\n",
      "word error on validation data is 100\n",
      "Time since we began trainiing [0m 7s]\n",
      "ctc Cost on validation data is 18.7938079834\n",
      "~~~~~~~~~~~~~~~~~~~~~~s==>s\n",
      "~~~~~~~~~~~~~~~~~~~~~~s==>s\n",
      "~~~~~~~~~~~~~~~~~~~~~~s==>s\n",
      "~~~~~~~~~~~~~~~~~~~~~~s==>s\n",
      "~~~~~~~~~~~~~~~~~~~~~~s==>s\n",
      "word error on validation data is 100\n",
      "Time since we began trainiing [0m 8s]\n",
      "ctc Cost on validation data is 16.8738975525\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "word error on validation data is 100\n",
      "Time since we began trainiing [0m 9s]\n",
      "ctc Cost on validation data is 16.943687439\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "word error on validation data is 100\n",
      "Time since we began trainiing [0m 11s]\n",
      "ctc Cost on validation data is 15.3015422821\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "word error on validation data is 100\n",
      "Time since we began trainiing [0m 12s]\n",
      "ctc Cost on validation data is 14.0915622711\n",
      "~~~~~~~~~~~~~~~~~~~~~~d==>d\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "b~~~~~~~~~~~~~~~~~~~~~~==>b\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "word error on validation data is 100\n",
      "Time since we began trainiing [0m 13s]\n",
      "ctc Cost on validation data is 12.937037468\n",
      "s~~~~~~~~~~~~~~~~~~l~~i==>sli\n",
      "h~~~~~~~~~~~~~~~~~~~~~~==>h\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "i~~~~~~~~~~~~~~~~t~~~~~==>it\n",
      "~~~~~~~~~~~~~~~~t~~~~~~==>t\n",
      "word error on validation data is 100\n",
      "Time since we began trainiing [0m 14s]\n",
      "ctc Cost on validation data is 10.2113437653\n",
      "c~~~~~~~~~~~~~~~~~~l~~i==>cli\n",
      "h~~~~~~~~~~~~~~~~~~~~~~==>h\n",
      "~~~~~~~~~~~~~~~~~~~~t~~==>t\n",
      "i~~~~~~~~~~~~~~~~t~~~~~==>it\n",
      "p~~~~~~~~~~~~~~~~~~~~~~==>p\n",
      "word error on validation data is 100\n",
      "Time since we began trainiing [0m 16s]\n",
      "ctc Cost on validation data is 7.18576669693\n",
      "a~~~~~~~~~~~~~~~~~~l~~i==>ali\n",
      "f~~~~~~~~~~~~~~~~~~~~~~==>f\n",
      "b~~~~~~e~~~~~e~~~~~~t~~==>beet\n",
      "i~a~~~~~~~~~a~~~~t~~a~~==>iaata\n",
      "p~~~~~a~~~~~~~~~t~~~~~~==>pat\n",
      "word error on validation data is 100\n",
      "Time since we began trainiing [0m 17s]\n",
      "ctc Cost on validation data is 5.03861188889\n",
      "a~~~~~~~~~a~~~~~~~~l~~i==>aali\n",
      "f~~~~~~~~o~~~~~n~~~~~~~==>fon\n",
      "b~~~~~e~~~~~~e~~~~~~t~~==>beet\n",
      "i~a~~~~~~~~~a~~~~t~~a~~==>iaata\n",
      "p~~~~~a~~~~~r~~~t~~~~~~==>part\n",
      "word error on validation data is 100\n",
      "Time since we began trainiing [0m 18s]\n",
      "ctc Cost on validation data is 4.24441623688\n",
      "a~~~~~~~~~a~~~~~~~~d~~i==>aadi\n",
      "f~~~r~~~o~~~~~~p~~~~~~~==>frop\n",
      "b~~~~~e~~~~~~e~~~~~~t~~==>beet\n",
      "i~a~~~~~~~~~a~~~~t~~a~~==>iaata\n",
      "p~~~~~a~~~~~r~~~t~~~~~~==>part\n",
      "word error on validation data is 100\n",
      "Time since we began trainiing [0m 20s]\n",
      "ctc Cost on validation data is 3.00456666946\n",
      "a~~~~~~~~~a~~~~~~~~d~~i==>aadi\n",
      "f~~~r~~~o~~~~~~n~~~~~~~==>fron\n",
      "b~~~~~e~~~~~~e~~~~~~f~~==>beef\n",
      "i~a~~~~n~~~~a~~~~t~~a~~==>ianata\n",
      "p~~~~~a~~~~~r~~~t~~~~~~==>part\n",
      "word error on validation data is 80\n",
      "Time since we began trainiing [0m 21s]\n",
      "ctc Cost on validation data is 2.26541376114\n",
      "a~~~~~~~~~a~~~~~~~~d~~i==>aadi\n",
      "f~~~r~~~o~~~~~~p~~~~~~~==>frop\n",
      "b~~~~~e~~~~~~e~~~~~~f~~==>beef\n",
      "i~a~~~~n~~~~a~~~~t~~a~~==>ianata\n",
      "p~~~~~a~~~~~r~~~t~~~~~~==>part\n",
      "word error on validation data is 80\n",
      "Time since we began trainiing [0m 22s]\n",
      "ctc Cost on validation data is 1.98987984657\n",
      "a~~~~~~~~~a~~~~~~~~d~~i==>aadi\n",
      "f~~~r~~~o~~~~~~m~~~~~~~==>from\n",
      "b~~~~~e~~~~~~e~~~~~~f~~==>beef\n",
      "i~a~~~~n~~~~a~~~~t~~a~~==>ianata\n",
      "p~~~~~a~~~~~r~~~t~~~y~~==>party\n",
      "word error on validation data is 40\n",
      "Time since we began trainiing [0m 23s]\n",
      "ctc Cost on validation data is 1.62726283073\n",
      "a~~~~~~~~~a~~~~~~~~d~~i==>aadi\n",
      "f~~~r~~~o~~~~~~m~~~~~~~==>from\n",
      "b~~~~~e~~~~~~e~~~~~~f~~==>beef\n",
      "i~a~~~~n~~~~a~~~~t~~a~~==>ianata\n",
      "p~~~~~a~~~~~r~~~t~~~y~~==>party\n",
      "word error on validation data is 40\n",
      "Time since we began trainiing [0m 25s]\n",
      "ctc Cost on validation data is 1.60770833492\n",
      "a~~~~~~~~~a~~~~~~~~d~~i==>aadi\n",
      "f~~~r~~~~o~~~~~m~~~~~~~==>from\n",
      "b~~~~~e~~~~~~e~~~~~~f~~==>beef\n",
      "i~a~~~~n~~~~a~~~~t~~a~~==>ianata\n",
      "p~~~~~a~~~~~r~~~t~~~y~~==>party\n",
      "word error on validation data is 40\n",
      "Time since we began trainiing [0m 26s]\n",
      "ctc Cost on validation data is 0.915098309517\n",
      "a~~~~z~~~~a~~~~~~~~d~~i==>azadi\n",
      "f~~~r~~~o~~~~~~m~~~~~~~==>from\n",
      "b~~~~~e~~~~~~e~~~~~~f~~==>beef\n",
      "i~a~~~~n~~~~a~~~~t~~a~~==>ianata\n",
      "p~~~~~a~~~~~r~~~t~~~y~~==>party\n",
      "word error on validation data is 20\n",
      "Time since we began trainiing [0m 27s]\n",
      "ctc Cost on validation data is 0.775833249092\n",
      "a~~~~z~~~~a~~~~~~~~d~~i==>azadi\n",
      "f~~~r~~~o~~~~~~m~~~~~~~==>from\n",
      "b~~~~~e~~~~~~e~~~~~~f~~==>beef\n",
      "j~a~~~~n~~~~a~~~~t~~a~~==>janata\n",
      "p~~~~~a~~~~~r~~~t~~~y~~==>party\n",
      "word error on validation data is 0\n",
      "Time since we began trainiing [0m 29s]\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "# TRAINING\n",
    "##################################################\n",
    "\"\"\"\n",
    "a batch of words are sequentially fetched from the vocabulary\n",
    "one epoch runs until all the words in the vocabulary are seen once\n",
    "then the word list is shuffled and above process is repeated\n",
    "\"\"\"\n",
    "nHidden=80\n",
    "batchSize=5 #if you have more gpu memory you may increase it and your training will be faster\n",
    "nClasses= len(alphabet)\n",
    "criterion = CTCLoss()\n",
    "\n",
    "numLayers=2 # the 2 BLSTM layers defined seprately without using numLayers option for nn.LSTM\n",
    "numDirections=2 # 2 since we need to use a bidirectional LSTM\n",
    "model = crnnocr(imHeight,nHidden,nClasses,numLayers,numDirections)\n",
    "\n",
    "optimizer=optim.Adam(model.parameters(), lr=0.001)\n",
    "start = time.time()\n",
    "if use_cuda:\n",
    "    model=model.cuda()\n",
    "    criterion=criterion.cuda()\n",
    "\n",
    "\n",
    "for iter in range (0,200):\n",
    "    avgTrainCost=0\n",
    "    random.shuffle(words)\n",
    "\n",
    "    for i in range (0,vocabSize-batchSize+1,batchSize):\n",
    "    \n",
    "        model.zero_grad()\n",
    "        \n",
    "        batchOfWords=words[i:i+batchSize]\n",
    "        images,labelSeqs,labelSeqlens =GetBatch(batchOfWords)\n",
    "        images=autograd.Variable(images)\n",
    "        #images=autograd.Variable(images)\n",
    "        images=images.contiguous()\n",
    "        if use_cuda:\n",
    "            images=images.cuda()\n",
    "        labelSeqs=autograd.Variable(labelSeqs)\n",
    "\n",
    "        labelSeqlens=autograd.Variable(labelSeqlens)\n",
    "        outputs=model(images)\n",
    "        outputs=outputs.contiguous()\n",
    "        outputsSize=autograd.Variable(torch.IntTensor([outputs.size(0)] * batchSize))\n",
    "        trainCost = criterion(outputs, labelSeqs, outputsSize, labelSeqlens) / batchSize\n",
    "\n",
    "        avgTrainCost+=trainCost\n",
    "        if i%500==0:\n",
    "            avgTrainCost=avgTrainCost/(5000/batchSize)\n",
    "            #print ('avgTraincost for last 5000 samples is',avgTrainCost)\n",
    "            avgTrainCost=0\n",
    "            valOutputs=model(valImages)\n",
    "#print (valOutputs.size()) 100 X nvalsamoles x 37\n",
    "            valOutputs=valOutputs.contiguous()\n",
    "            valOutputsSize=autograd.Variable(torch.IntTensor([valOutputs.size(0)] * len(valWords)))\n",
    "            valCost=criterion(valOutputs, valLabelSeqs, valOutputsSize, valLabelSeqlens) / len(valWords)\n",
    "            print ('ctc Cost on validation data is',valCost.data[0])\n",
    "            if valCost.data[0] < 0.3:\n",
    "                sys.exit()\n",
    "\n",
    "\n",
    "            ### get the actual predictions and compute word error ################\n",
    "            valOutputs=valOutputs.transpose(0,1)\n",
    "            # second output of max() is the argmax along the requuired dimension\n",
    "            _, argMaxActivations= valOutputs.max(2)\n",
    "            #the below tensor each raw is the sequences of labels predicted for each sample in the batch\n",
    "            predictedSeqLabels=argMaxActivations.squeeze(2) #batchSize * seqLen\n",
    "            predictedRawStrings,predictedStrings=Labels2Str(predictedSeqLabels)\n",
    "            nCorrectWords=0\n",
    "            WER=100\n",
    "            for ii in range(0,5):\n",
    "\n",
    "                print (predictedRawStrings[ii]+\"==>\"+predictedStrings[ii])\n",
    "                if predictedStrings[ii]==valWords[ii]:\n",
    "                    nCorrectWords+=1\n",
    "            WER=((len(valWords)-nCorrectWords)*100)/len(valWords)\n",
    "            print ('word error on validation data is', WER)\n",
    "            \n",
    "\n",
    "            #   print (predictedSeqLabels[0,:].transpose(0,0))\n",
    "            #print(valOutputs_batchFirst[0,0,:])\n",
    "            print('Time since we began trainiing [%s]' % (time_since(start)))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        trainCost.backward()\n",
    "        optimizer.step()\n",
    "        if WER == 0:\n",
    "            break\n",
    "    if WER==0:\n",
    "        break\n",
    "    \n",
    "    #iterString=int(iter)\n",
    "    #torch.save(model.state_dict(), iterString+'.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
