{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using RNNs to add two binary strings ##\n",
    "if  two input  binary strings say 010 and 011 are given your network should output the sum = 101\n",
    "\n",
    "- How do you represent the data \n",
    "- Defining a simple recurrent network to model the problem in a seq2seq fashion\n",
    "\n",
    "- Train it on binary strings of a fixed length\n",
    "- Test the network using binary strings of different lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "# =============================================================================\n",
    "# Make a simple RNN learn binray addition \n",
    "\n",
    "# ============================================================================\n",
    "# author  mineshmathew.github.io\n",
    "\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "random.seed( 10 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the input data ##\n",
    "\n",
    "###   Radom binary strings of required length as training data ###\n",
    "- The  function <i>getSample()</i> takes a string-length as input and then returns the input vector and target vector that need to be fed to the RNN\n",
    "- Say if your string-length is 2, lower and upper bounds would be 2 and 3. \n",
    "- Then if the two random numbers picked from this range are 2 and 3 ( you have only 2 and 3 in that range :) )\n",
    "- your inputs in binary would be 10 and 11 and your sum is 5 which is 101.\n",
    "- <b> Padding :</b>Since your ouput is one bit longer we will rewrite the inputs too in 3 bit form so  010 + 011 -- > 101\n",
    "\n",
    "\n",
    "### Training data as input sequene and target sequence pairs  ###\n",
    "Starting from the least significant bit  (since the addition starts from LSB) we concatenate the correspodning bits in each input binary string and that forms our input sequence.\n",
    "And your target vector would be the ourput binary string reversed (Since you start from LSB)\n",
    "\n",
    "Hence  your input at one timestep is this ordered pair of bits for that particular position and target for that timestep would be the corresponding bit in the output string\n",
    "\n",
    "so your input dimension at each time step is 2 and target dimesnion is 1\n",
    "\n",
    "in the above case so your input and target pairs would be\n",
    "\n",
    "[1 0] - > 1 <br>\n",
    "[1 1]  -> 0 <br>\n",
    "[0 0]  -> 0\n",
    "\n",
    "\n",
    "![title](binaryinput.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSample(stringLength, testFlag):\n",
    "  #takes stringlength as input \n",
    "  #returns a sample for the network - an input sequence - x and its target -y\n",
    "  #x is a T*2 array, T is the length of the string and 2 since we take one bit each from each string\n",
    "  #testFlag if set prints the input numbers and its sum in both decimal and binary form\n",
    "  lowerBound=pow(2,stringLength-1)+1\n",
    "  upperBound=pow(2,stringLength)\n",
    "\n",
    "  num1=random.randint(lowerBound,upperBound)\n",
    "  num2=random.randint(lowerBound,upperBound)\n",
    "\n",
    "  num3=num1+num2\n",
    "  num3Binary=(bin(num3)[2:])\n",
    "\n",
    "  num1Binary=(bin(num1)[2:])\n",
    "\n",
    "  num2Binary=(bin(num2)[2:])\n",
    "\n",
    "  if testFlag==1:\n",
    "    print('input numbers and their sum  are', num1, ' ', num2, ' ', num3)\n",
    "    print ('binary strings are', num1Binary, ' ' , num2Binary, ' ' , num3Binary)\n",
    "  len_num1= (len(num1Binary))\n",
    "\n",
    "  len_num2= (len(num2Binary))\n",
    "  len_num3= (len(num3Binary))\n",
    "\n",
    "  # since num3 will be the largest, we pad  other numbers with zeros to that num3_len\n",
    "  num1Binary= ('0'*(len(num3Binary)-len(num1Binary))+num1Binary)\n",
    "  num2Binary= ('0'*(len(num3Binary)-len(num2Binary))+num2Binary)\n",
    "\n",
    "\n",
    "  # forming the input sequence\n",
    "  # the input at first timestep is the least significant bits of the two input binary strings\n",
    "  # x will be then a len_num3 ( or T ) * 2 array\n",
    "  x=np.zeros((len_num3,2),dtype=np.float32)\n",
    "  for i in range(0, len_num3):\n",
    "    x[i,0]=num1Binary[len_num3-1-i] # note that MSB of the binray string should be the last input along the time axis\n",
    "    x[i,1]=num2Binary[len_num3-1-i]\n",
    "  # target vector is the sum in binary\n",
    "  # convert binary string in <string> to a numpy 1D array\n",
    "  #https://stackoverflow.com/questions/29091869/convert-bitstring-string-of-1-and-0s-to-numpy-array\n",
    "  y=np.array(map(int, num3Binary[::-1]))\n",
    "  #print (x)\n",
    "  #print (y)\n",
    "  return x,y \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the network look like ? ##\n",
    "The figure below shows  fully rolled network for our task for the input - target pair we took as an example earlier.\n",
    "In the figure, for ease of drawing, hiddenDIm is chosen as 2\n",
    "![network architecture](binarynet.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Adder (nn.Module):\n",
    "  def __init__(self, inputDim, hiddenDim, outputDim):\n",
    "    super(Adder, self).__init__()\n",
    "    self.inputDim=inputDim\n",
    "    self.hiddenDim=hiddenDim\n",
    "    self.outputDim=outputDim\n",
    "    self.lstm=nn.RNN(inputDim, hiddenDim )\n",
    "    self.outputLayer=nn.Linear(hiddenDim, outputDim)\n",
    "    self.sigmoid=nn.Sigmoid()\n",
    "  def forward(self, x ):\n",
    "    #size of x is T x B x featDim\n",
    "    #B=1 is dummy batch dimension added, because pytorch mandates it\n",
    "    #if you want B as first dimension of x then specift batchFirst=True when LSTM is initalized\n",
    "    #T,D  = x.size(0), x.size(1)\n",
    "    #batch is a must \n",
    "    lstmOut,_ =self.lstm(x ) #x has two  dimensions  seqLen *batch* FeatDim=2\n",
    "    T,B,D  = lstmOut.size(0),lstmOut.size(1) , lstmOut.size(2)\n",
    "    lstmOut = lstmOut.contiguous() \n",
    "        # before  feeding to linear layer we squash one dimension\n",
    "    lstmOut = lstmOut.view(B*T, D)\n",
    "    outputLayerActivations=self.outputLayer(lstmOut)\n",
    "    #reshape actiavtions to T*B*outputlayersize\n",
    "    outputLayerActivations=outputLayerActivations.view(T,B,-1).squeeze(1)\n",
    "    outputSigmoid=self.sigmoid(outputLayerActivations)\n",
    "    return outputSigmoid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### traning the network ###\n",
    "\n",
    "- batch learning is not used, only one seqeuence is fed at a time\n",
    "- runs purely on a cpu\n",
    "- MSE loss is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model initialized\n",
      " Avg. Loss for last 500 samples = inf\n",
      " Avg. Loss for last 500 samples = 0.223358\n",
      " Avg. Loss for last 500 samples = 0.206651\n",
      " Avg. Loss for last 500 samples = 0.199365\n",
      " Avg. Loss for last 500 samples = 0.196121\n",
      " Avg. Loss for last 500 samples = 0.196293\n",
      " Avg. Loss for last 500 samples = 0.191138\n",
      " Avg. Loss for last 500 samples = 0.190323\n",
      " Avg. Loss for last 500 samples = 0.185904\n",
      " Avg. Loss for last 500 samples = 0.178209\n",
      " Avg. Loss for last 500 samples = 0.170732\n",
      " Avg. Loss for last 500 samples = 0.156556\n",
      " Avg. Loss for last 500 samples = 0.140215\n",
      " Avg. Loss for last 500 samples = 0.119895\n",
      " Avg. Loss for last 500 samples = 0.092916\n",
      " Avg. Loss for last 500 samples = 0.070904\n",
      " Avg. Loss for last 500 samples = 0.050976\n",
      " Avg. Loss for last 500 samples = 0.034916\n",
      " Avg. Loss for last 500 samples = 0.023986\n",
      " Avg. Loss for last 500 samples = 0.016859\n",
      " Avg. Loss for last 500 samples = 0.011595\n",
      " Avg. Loss for last 500 samples = 0.008252\n",
      " Avg. Loss for last 500 samples = 0.006067\n",
      " Avg. Loss for last 500 samples = 0.004405\n",
      " Avg. Loss for last 500 samples = 0.003159\n",
      " Avg. Loss for last 500 samples = 0.002342\n",
      " Avg. Loss for last 500 samples = 0.001734\n",
      " Avg. Loss for last 500 samples = 0.001248\n",
      " Avg. Loss for last 500 samples = 0.000943\n",
      " Avg. Loss for last 500 samples = 0.000708\n",
      " Avg. Loss for last 500 samples = 0.000535\n",
      " Avg. Loss for last 500 samples = 0.000402\n",
      " Avg. Loss for last 500 samples = 0.000303\n",
      " Avg. Loss for last 500 samples = 0.000238\n",
      " Avg. Loss for last 500 samples = 0.000174\n",
      " Avg. Loss for last 500 samples = 0.000133\n",
      " Avg. Loss for last 500 samples = 0.000102\n",
      " Avg. Loss for last 500 samples = 0.000078\n",
      " Avg. Loss for last 500 samples = 0.000058\n",
      " Avg. Loss for last 500 samples = 0.000046\n",
      " Avg. Loss for last 500 samples = 0.000035\n",
      " Avg. Loss for last 500 samples = 0.000027\n",
      " Avg. Loss for last 500 samples = 0.000021\n",
      " Avg. Loss for last 500 samples = 0.000016\n",
      " Avg. Loss for last 500 samples = 0.000012\n"
     ]
    }
   ],
   "source": [
    "\n",
    "featDim=2 # two bits each from each of the String\n",
    "outputDim=1 # one output node which would output a zero or 1\n",
    "\n",
    "lstmSize=10\n",
    "\n",
    "lossFunction = nn.MSELoss()\n",
    "model =Adder(featDim, lstmSize, outputDim)\n",
    "print ('model initialized')\n",
    "#optimizer = optim.SGD(model.parameters(), lr=3e-2, momentum=0.8)\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.001)\n",
    "epochs=500\n",
    "### epochs ##\n",
    "totalLoss= float(\"inf\")\n",
    "while totalLoss > 1e-5:\n",
    "  print(\" Avg. Loss for last 500 samples = %lf\"%(totalLoss))\n",
    "  totalLoss=0\n",
    "  for i in range(0,epochs): # average the loss over 200 samples\n",
    "    \n",
    "    stringLen=4\n",
    "    testFlag=0\n",
    "    x,y=getSample(stringLen, testFlag)\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "\n",
    "    x_var=autograd.Variable(torch.from_numpy(x).unsqueeze(1).float()) #convert to torch tensor and variable\n",
    "    # unsqueeze() is used to add the extra dimension since\n",
    "    # your input need to be of t*batchsize*featDim; you cant do away with the batch in pytorch\n",
    "    seqLen=x_var.size(0)\n",
    "    #print (x_var)\n",
    "    x_var= x_var.contiguous()\n",
    "    y_var=autograd.Variable(torch.from_numpy(y).float())\n",
    "    finalScores = model(x_var)\n",
    "    #finalScores=finalScores.\n",
    "\n",
    "    loss=lossFunction(finalScores,y_var)  \n",
    "    totalLoss+=loss.data[0]\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "  totalLoss=totalLoss/epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model ###\n",
    "Remember that the network was purely trained on strings of length =3 <br>\n",
    "now lets the net on bitstrings of length=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input numbers and their sum  are 26   26   52\n",
      "binary strings are 11010   11010   110100\n",
      "sum predicted by RNN is  [1 1 0 1 0 0]\n",
      "##################################################\n",
      "input numbers and their sum  are 27   24   51\n",
      "binary strings are 11011   11000   110011\n",
      "sum predicted by RNN is  [1 1 0 0 1 1]\n",
      "##################################################\n",
      "input numbers and their sum  are 25   22   47\n",
      "binary strings are 11001   10110   101111\n",
      "sum predicted by RNN is  [1 0 1 1 1 1]\n",
      "##################################################\n",
      "input numbers and their sum  are 17   23   40\n",
      "binary strings are 10001   10111   101000\n",
      "sum predicted by RNN is  [1 0 1 0 0 0]\n",
      "##################################################\n",
      "input numbers and their sum  are 22   23   45\n",
      "binary strings are 10110   10111   101101\n",
      "sum predicted by RNN is  [1 0 1 1 0 1]\n",
      "##################################################\n",
      "input numbers and their sum  are 31   27   58\n",
      "binary strings are 11111   11011   111010\n",
      "sum predicted by RNN is  [1 1 1 0 1 0]\n",
      "##################################################\n",
      "input numbers and their sum  are 24   26   50\n",
      "binary strings are 11000   11010   110010\n",
      "sum predicted by RNN is  [1 1 0 0 1 0]\n",
      "##################################################\n",
      "input numbers and their sum  are 23   31   54\n",
      "binary strings are 10111   11111   110110\n",
      "sum predicted by RNN is  [1 1 0 1 1 0]\n",
      "##################################################\n",
      "input numbers and their sum  are 28   20   48\n",
      "binary strings are 11100   10100   110000\n",
      "sum predicted by RNN is  [1 1 0 0 0 0]\n",
      "##################################################\n",
      "input numbers and their sum  are 19   32   51\n",
      "binary strings are 10011   100000   110011\n",
      "sum predicted by RNN is  [1 1 0 0 1 1]\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "stringLen=5\n",
    "testFlag=1\n",
    "# test the network on 10 random binary string addition cases where stringLen=4\n",
    "for i in range (0,10):\n",
    "\tx,y=getSample(stringLen,testFlag)\n",
    "\tx_var=autograd.Variable(torch.from_numpy(x).unsqueeze(1).float())\n",
    "\ty_var=autograd.Variable(torch.from_numpy(y).float())\n",
    "\tseqLen=x_var.size(0)\n",
    "\tx_var= x_var.contiguous()\n",
    "\tfinalScores = model(x_var).data.t()\n",
    "\t#print(finalScores)\n",
    "\tbits=finalScores.gt(0.5)\n",
    "\tbits=bits[0].numpy()\n",
    "\n",
    "\tprint ('sum predicted by RNN is ',bits[::-1])\n",
    "\tprint('##################################################')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Things to try out \n",
    "- See that increasing the hidden size to say 100 worsens the performance\n",
    "- Change the model slightly to use NLL loss or cross entropy loss (you may want to add two output nodes in this case; one for 1 and one for 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
